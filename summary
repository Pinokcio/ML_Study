supervised learning - learning with labeled examples
- regressing
- binary classification
- multi-label classification

un-supervised learning - grouping, word clustering

hypothesis : H(x) = Wx + b  

cost : H(x) - y , 가설과 실제 데이터값의 차이를 나타냄. 보통 cost에 제곱을 함으로써 cost가 클수록 더 큰 가중치를 줌. 
일반적으로 전체 데이터에 대한 cost의 평균값을 구하여 사용.

@ Goal : cost의 minimize가 learning의 최종 목표임

linear regression의 cost function을 최소화 하는 방법 : Gradient discent algorithm

Gradient discent algorithm - W : W - a(d/dW)cost(W) , a = learning rate , Convex function(경사를 따라 내려갔을 때 한점으로 모이게 되는 함수형태)에 사용됨

sigmoid = 1/1_e*-z
relu = max(0, 1)
* back propagation에서 sigmoid를 사용할 경우 y의 범위가 0~1이므로 여러번 곱해지면 한없이 작아질 수 있다. 이는 vanishing gradient를 야기할 수 있으므로
relu를 이용하여 해당 layer의 영향을 줄임으로써 더 좋은 결과를 얻을 수 있도록 한다.

drop out : 딥러닝 모델에서 여러 레이어가 있을 때 어떠한 레이어에서 특정한 값의 영향을 줄임으로써 오버피팅(과적합)의 문제를 해결한다.

앙상블 : 같은 데이터에 대해서 여러모델을 만들어 합치는 기법.
- 배깅 : 병렬 처리, 부트스트랩을 통해 생성한 데이터를 여러 모델을 통해 학습하고 특정 데이터에 대한 결과값을 투표를 통해 결정함.
- 부스팅 : 직렬 처리, 모델 1에 대한 결과에 따라 가중치를 주고, 이러한 과정을 모델 2,3,...,n 까지 반복하여 결과값을 출력함.

image classification

image captioning

object detection
